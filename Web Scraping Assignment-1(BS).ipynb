{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3798ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca70720",
   "metadata": {},
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c31a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header tags :\n",
      "\n",
      "<h1 class=\"central-textlogo-wrapper\">\n",
      "<span class=\"central-textlogo__image sprite svg-Wikipedia_wordmark\">\n",
      "Wikipedia\n",
      "</span>\n",
      "<strong class=\"jsl10n localized-slogan\" data-jsl10n=\"portal.slogan\">The Free Encyclopedia</strong>\n",
      "</h1>\n",
      "\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "1 000 000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "100 000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "10 000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "1 000+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n",
      "\n",
      "<h2 class=\"bookshelf-container\">\n",
      "<span class=\"bookshelf\">\n",
      "<span class=\"text\">\n",
      "<bdi dir=\"ltr\">\n",
      "100+\n",
      "</bdi>\n",
      "<span class=\"jsl10n\" data-jsl10n=\"portal.entries\">\n",
      "articles\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</h2>\n"
     ]
    }
   ],
   "source": [
    "page = requests.get('https://www.wikipedia.org/')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "titles = soup.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "print('Header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138e3e6",
   "metadata": {},
   "source": [
    "# 2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)and make data frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b02866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb(url):\n",
    "    Names=[]\n",
    "    Ratings=[]\n",
    "    Years=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    name=soup.find_all('td',class_=\"titleColumn\")\n",
    "    for i in name:\n",
    "        full_name=i.find('a').get_text()\n",
    "        Names.append(full_name)\n",
    "    rating=soup.find_all('td',class_=\"ratingColumn imdbRating\")\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text.replace('\\n',''))\n",
    "    year=soup.find_all('span',class_=\"secondaryInfo\")\n",
    "    for i in year:\n",
    "        Years.append(i.text.replace('(','').replace(')',''))\n",
    "    import pandas as pd\n",
    "    movies=pd.DataFrame({})\n",
    "    movies['Names']=Names\n",
    "    movies['Ratings']=Ratings\n",
    "    movies['Year of Release']=Years\n",
    "    movies=movies[:100]\n",
    "    print(movies)\n",
    "    movies.to_csv('Top_100_imdb_movies.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225f0106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Names Ratings Year of Release\n",
      "0              The Shawshank Redemption     9.2            1994\n",
      "1                         The Godfather     9.2            1972\n",
      "2                       The Dark Knight     9.0            2008\n",
      "3                 The Godfather Part II     9.0            1974\n",
      "4                          12 Angry Men     8.9            1957\n",
      "..                                  ...     ...             ...\n",
      "95                               Jagten     8.3            2012\n",
      "96    M - Eine Stadt sucht einen Mörder     8.3            1931\n",
      "97                   North by Northwest     8.3            1959\n",
      "98                              Vertigo     8.2            1958\n",
      "99  Le fabuleux destin d'Amélie Poulain     8.2            2001\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "imdb('https://www.imdb.com/chart/top/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1758b53",
   "metadata": {},
   "source": [
    "# 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145a35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_india(url):\n",
    "    Names=[]\n",
    "    Ratings=[]\n",
    "    Years=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    name=soup.find_all('td',class_=\"titleColumn\")\n",
    "    for i in name:\n",
    "        full_name=i.find('a').get_text()\n",
    "        Names.append(full_name)\n",
    "    rating=soup.find_all('td',class_=\"ratingColumn imdbRating\")\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text.replace('\\n',''))\n",
    "    year=soup.find_all('span',class_=\"secondaryInfo\")\n",
    "    for i in year:\n",
    "        Years.append(i.text.replace('(','').replace(')',''))\n",
    "    import pandas as pd\n",
    "    movies=pd.DataFrame({})\n",
    "    movies['Names']=Names\n",
    "    movies['Ratings']=Ratings\n",
    "    movies['Year of Release']=Years\n",
    "    movies=movies[:100]\n",
    "    print(movies)\n",
    "    movies.to_csv('Top_100_imdb_indian_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b00ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Names Ratings Year of Release\n",
      "0                      Jai Bhim     8.4            2021\n",
      "1                    Anbe Sivam     8.4            2003\n",
      "2                       Golmaal     8.4            1979\n",
      "3                       Nayakan     8.4            1987\n",
      "4             Pariyerum Perumal     8.4            2018\n",
      "..                          ...     ...             ...\n",
      "95                       Masaan     8.0            2015\n",
      "96                      Kahaani     8.0            2012\n",
      "97             K.G.F: Chapter 2     8.0            2022\n",
      "98  Baahubali 2: The Conclusion     8.0            2017\n",
      "99               Dil Chahta Hai     8.0            2001\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "imdb_india('https://www.imdb.com/india/top-rated-indian-movies/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539dcc18",
   "metadata": {},
   "source": [
    "# 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec9964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef8f399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Term of office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0          Shri Pranab Mukherjee    \n",
       "1   Smt Pratibha Devisingh Patil    \n",
       "2         DR. A.P.J. Abdul Kalam    \n",
       "3           Shri K. R. Narayanan    \n",
       "4        Dr Shankar Dayal Sharma    \n",
       "5            Shri R Venkataraman    \n",
       "6               Giani Zail Singh    \n",
       "7      Shri Neelam Sanjiva Reddy    \n",
       "8       Dr. Fakhruddin Ali Ahmed    \n",
       "9   Shri Varahagiri Venkata Giri    \n",
       "10              Dr. Zakir Husain    \n",
       "11  Dr. Sarvepalli Radhakrishnan    \n",
       "12           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of office  \n",
       "0                     25 July, 2012 to 25 July, 2017   \n",
       "1                     25 July, 2007 to 25 July, 2012   \n",
       "2                     25 July, 2002 to 25 July, 2007   \n",
       "3                     25 July, 1997 to 25 July, 2002   \n",
       "4                     25 July, 1992 to 25 July, 1997   \n",
       "5                     25 July, 1987 to 25 July, 1992   \n",
       "6                     25 July, 1982 to 25 July, 1987   \n",
       "7                     25 July, 1977 to 25 July, 1982   \n",
       "8                24 August, 1974 to 11 February, 1977  \n",
       "9    3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "10                        13 May, 1967 to 3 May, 1969  \n",
       "11                       13 May, 1962 to 13 May, 1967  \n",
       "12                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "import pandas as pd\n",
    "\n",
    "Name = []\n",
    "Term = []\n",
    "\n",
    "name = soup.find_all('div', class_ = 'presidentListing')\n",
    "for i in name:\n",
    "    full_name = i.find('h3').text.split(\"(\")[0]\n",
    "    Name.append(full_name)\n",
    "\n",
    "term = soup.find_all('div', class_ = 'presidentListing')\n",
    "for i in term:\n",
    "    term = i.find('p').text.split(\"Term of Office:\")[1]\n",
    "    Term.append(term)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Name'] = Name\n",
    "df['Term of office'] = Term\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133212e",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420f4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 ODI Teams - Mens\n",
    "\n",
    "def top_10_teams(url):\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    \n",
    "    team=soup.find_all('span',class_='u-hide-phablet')\n",
    "    for i in team:\n",
    "        teams.append(i.text)\n",
    "    teams=teams[:10]\n",
    "    #I observed that top row style is different and remaining rows are different\n",
    "    \n",
    "    # For top row:\n",
    "    match_top_row=soup.find_all('td',class_='rankings-block__banner--matches')    \n",
    "    for i in match_top_row:\n",
    "        matches.append(i.text)\n",
    "    # For remaining 9 teams\n",
    "    match=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    # I am using even indices only for Matches. Will use odd ones for Points.\n",
    "    for i in range(0,len(match),2):\n",
    "        matches.append(match[i].text)\n",
    "    matches=matches[:10]\n",
    "    \n",
    "    point_top_row=soup.find_all('td',class_='rankings-block__banner--points')\n",
    "    #Similarly, I observed that top row style is different and remaining rows are different\n",
    "    for i in point_top_row:\n",
    "        points.append(i.text)\n",
    "    # For remaining 9 teams\n",
    "    point=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    # I am using odd indices only for Matches. Will use odd ones for Points.\n",
    "    for i in range(1,len(match),2):\n",
    "        points.append(point[i].text)\n",
    "    points=points[:10]\n",
    "    \n",
    "    rating=soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    #Similarly, I observed that top row style is different and remaining rows are different\n",
    "    for i in rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    # For remaining 9 teams\n",
    "    rating=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    teams_odi=pd.DataFrame({})\n",
    "    teams_odi['teams']=teams\n",
    "    teams_odi['matches']=matches\n",
    "    teams_odi['points']=points\n",
    "    teams_odi['ratings']=ratings\n",
    "    return(teams_odi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1dc6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teams</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>12</td>\n",
       "      <td>1,505</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>22</td>\n",
       "      <td>2,756</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>19</td>\n",
       "      <td>2,005</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>22</td>\n",
       "      <td>2,304</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>23</td>\n",
       "      <td>2,325</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>19</td>\n",
       "      <td>1,872</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>24</td>\n",
       "      <td>2,275</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>32</td>\n",
       "      <td>2,306</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          teams matches points ratings\n",
       "0   New Zealand      12  1,505     125\n",
       "1       England      22  2,756     125\n",
       "2      Pakistan      19  2,005     106\n",
       "3         India      22  2,304     105\n",
       "4     Australia      23  2,325     101\n",
       "5  South Africa      19  1,872      99\n",
       "6    Bangladesh      24  2,275      95\n",
       "7     Sri Lanka      29  2,658      92\n",
       "8   West Indies      32  2,306      72\n",
       "9   Afghanistan      18  1,238      69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_10_teams('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09d8e1",
   "metadata": {},
   "source": [
    "5. b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccdfe112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_Batsmen(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    #I have observed top row style is different and remaining rows are different\n",
    "    \n",
    "    #First row for Batsman\n",
    "    player=soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in player:\n",
    "        players.append(i.text)\n",
    "\n",
    "        \n",
    "    #Next 9 rows for Batsman\n",
    "    player=soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "\n",
    "   \n",
    "    #First row for Team\n",
    "    team=soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "    #Next 9 rows for Team\n",
    "    team=soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))        \n",
    "    teams=teams[:10]\n",
    "    \n",
    "       \n",
    "    # First row for Rating\n",
    "    rating=soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "    # Next 9 rows for Rating\n",
    "    rating=soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "\n",
    "        \n",
    "    import pandas as pd\n",
    "    batsmen_odi=pd.DataFrame({})\n",
    "    batsmen_odi['players']=players\n",
    "    batsmen_odi['teams']=teams\n",
    "    batsmen_odi['ratings']=ratings\n",
    "    return(batsmen_odi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e162468f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>players</th>\n",
       "      <th>teams</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 players teams ratings\n",
       "0             Babar Azam   PAK     892\n",
       "1            Imam-ul-Haq   PAK     815\n",
       "2            Virat Kohli   IND     811\n",
       "3           Rohit Sharma   IND     791\n",
       "4        Quinton de Kock    SA     789\n",
       "5            Ross Taylor    NZ     775\n",
       "6  Rassie van der Dussen    SA     769\n",
       "7         Jonny Bairstow   ENG     760\n",
       "8           David Warner   AUS     745\n",
       "9            Aaron Finch   AUS     727"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_10_Batsmen('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4d510",
   "metadata": {},
   "source": [
    "5. c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda3fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_Bowler(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    #I have observed top row style is different and remaining rows are different\n",
    "    \n",
    "    #First row for Bowler\n",
    "    player=soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in player:\n",
    "        players.append(i.text)\n",
    "\n",
    "        \n",
    "    #Next 9 rows for Bowler\n",
    "    player=soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "\n",
    "   \n",
    "    #First row for Team\n",
    "    team=soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "    #Next 9 rows for Team\n",
    "    team=soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))        \n",
    "    teams=teams[:10]\n",
    "    \n",
    "       \n",
    "    # First row for Rating\n",
    "    rating=soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "    # Next 9 rows for Rating\n",
    "    rating=soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "\n",
    "        \n",
    "    import pandas as pd\n",
    "    Bowler_odi=pd.DataFrame({})\n",
    "    Bowler_odi['players']=players\n",
    "    Bowler_odi['teams']=teams\n",
    "    Bowler_odi['ratings']=ratings\n",
    "    return(Bowler_odi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82076118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>players</th>\n",
       "      <th>teams</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shakib Al Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            players teams ratings\n",
       "0       Trent Boult    NZ     726\n",
       "1      Chris Woakes   ENG     686\n",
       "2        Matt Henry    NZ     683\n",
       "3    Shaheen Afridi   PAK     681\n",
       "4    Jasprit Bumrah   IND     679\n",
       "5  Mujeeb Ur Rahman   AFG     676\n",
       "6    Josh Hazlewood   AUS     667\n",
       "7      Mehedi Hasan   BAN     661\n",
       "8     Mohammad Nabi   AFG     657\n",
       "9   Shakib Al Hasan   BAN     657"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_10_Bowler('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d60bb6",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278d78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_womens_teams(url):\n",
    "    teams=[]\n",
    "    matches=[]\n",
    "    points=[]\n",
    "    ratings=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    \n",
    "    team=soup.find_all('span',class_='u-hide-phablet')\n",
    "    for i in team:\n",
    "        teams.append(i.text)\n",
    "    teams=teams[:10]\n",
    "    \n",
    "    #I observed that top row style is different and remaining rows are different\n",
    "    \n",
    "    match_top_row=soup.find_all('td',class_='rankings-block__banner--matches')    \n",
    "    for i in match_top_row:\n",
    "        matches.append(i.text)\n",
    "    # For remaining 9 teams\n",
    "    match=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    # I am using even indices only for Matches. Will use odd ones for Points.\n",
    "    for i in range(0,len(match),2):\n",
    "        matches.append(match[i].text)\n",
    "    matches=matches[:10]\n",
    "    \n",
    "    point_top_row=soup.find_all('td',class_='rankings-block__banner--points')\n",
    "    #Similarly, I observed that top row style is different and remaining rows are different\n",
    "    for i in point_top_row:\n",
    "        points.append(i.text)\n",
    "    # For remaining 9 teams\n",
    "    point=soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "    # I am using odd indices only for Matches. Will use odd ones for Points.\n",
    "    for i in range(1,len(match),2):\n",
    "        points.append(point[i].text)\n",
    "    points=points[:10]\n",
    "    \n",
    "    rating=soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "    #Similarly, I observed that top row style is different and remaining rows are different\n",
    "    for i in rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    # For remaining 9 teams\n",
    "    rating=soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text.replace('\\n','').replace(' ',''))\n",
    "    ratings=ratings[:10]\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    teams_odi=pd.DataFrame({})\n",
    "    teams_odi['teams']=teams\n",
    "    teams_odi['matches']=matches\n",
    "    teams_odi['points']=points\n",
    "    teams_odi['ratings']=ratings\n",
    "    return(teams_odi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be2c797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teams</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>4,837</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>32</td>\n",
       "      <td>3,949</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,531</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>29</td>\n",
       "      <td>2,889</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>1,962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>384</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>8</td>\n",
       "      <td>351</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          teams matches points ratings\n",
       "0     Australia      29  4,837     167\n",
       "1  South Africa      32  3,949     123\n",
       "2       England      30  3,531     118\n",
       "3         India      29  2,889     100\n",
       "4   New Zealand      31  3,019      97\n",
       "5   West Indies      30  2,768      92\n",
       "6    Bangladesh      12    930      78\n",
       "7      Pakistan      30  1,962      65\n",
       "8     Sri Lanka       8    384      48\n",
       "9       Ireland       8    351      44"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_10_womens_teams('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831fac5",
   "metadata": {},
   "source": [
    "6. b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34a90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_women_Batsmen(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    #I have observed top row style is different and remaining rows are different\n",
    "    \n",
    "    #First row for Batsman\n",
    "    player=soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in player:\n",
    "        players.append(i.text)\n",
    "\n",
    "        \n",
    "    #Next 9 rows for Batsman\n",
    "    player=soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "\n",
    "   \n",
    "    #First row for Team\n",
    "    team=soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "    #Next 9 rows for Team\n",
    "    team=soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))        \n",
    "    teams=teams[:10]\n",
    "    \n",
    "       \n",
    "    # First row for Rating\n",
    "    rating=soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "    # Next 9 rows for Rating\n",
    "    rating=soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "\n",
    "        \n",
    "    import pandas as pd\n",
    "    batsmen_odi=pd.DataFrame({})\n",
    "    batsmen_odi['players']=players\n",
    "    batsmen_odi['teams']=teams\n",
    "    batsmen_odi['ratings']=ratings\n",
    "    return(batsmen_odi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db91ca3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>players</th>\n",
       "      <th>teams</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             players teams ratings\n",
       "0       Alyssa Healy   AUS     785\n",
       "1     Natalie Sciver   ENG     750\n",
       "2        Beth Mooney   AUS     748\n",
       "3    Laura Wolvaardt    SA     713\n",
       "4        Meg Lanning   AUS     710\n",
       "5     Rachael Haynes   AUS     701\n",
       "6  Amy Satterthwaite    NZ     681\n",
       "7    Smriti Mandhana   IND     669\n",
       "8     Tammy Beaumont   ENG     659\n",
       "9       Ellyse Perry   AUS     642"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_10_women_Batsmen('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33421541",
   "metadata": {},
   "source": [
    "6. c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c3fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_women_all_rounder(url):\n",
    "    players=[]\n",
    "    teams=[]\n",
    "    ratings=[]\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    #I have observed top row style is different and remaining rows are different\n",
    "    \n",
    "    #First row for All-rounder\n",
    "    player=soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "    for i in player:\n",
    "        players.append(i.text)\n",
    "\n",
    "        \n",
    "    #Next 9 rows for All-rounder\n",
    "    player=soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "    for i in player:\n",
    "        players.append(i.find('a').text)\n",
    "    players=players[:10]\n",
    "\n",
    "   \n",
    "    #First row for Team\n",
    "    team=soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))\n",
    "        \n",
    "    #Next 9 rows for Team\n",
    "    team=soup.find_all('span',class_='table-body__logo-text')\n",
    "    for i in team:\n",
    "        teams.append(i.text.replace('\\n',''))        \n",
    "    teams=teams[:10]\n",
    "    \n",
    "       \n",
    "    # First row for Rating\n",
    "    rating=soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "        \n",
    "    # Next 9 rows for Rating\n",
    "    rating=soup.find_all('td',class_='table-body__cell rating')\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "    ratings=ratings[:10]\n",
    "\n",
    "        \n",
    "    import pandas as pd\n",
    "    all_rounder_odi=pd.DataFrame({})\n",
    "    all_rounder_odi['players']=players\n",
    "    all_rounder_odi['teams']=teams\n",
    "    all_rounder_odi['ratings']=ratings\n",
    "    return(all_rounder_odi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc6f500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>players</th>\n",
       "      <th>teams</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sune Luus</td>\n",
       "      <td>SA</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            players teams ratings\n",
       "0    Natalie Sciver   ENG     393\n",
       "1      Ellyse Perry   AUS     374\n",
       "2   Hayley Matthews    WI     338\n",
       "3    Marizanne Kapp    SA     338\n",
       "4       Amelia Kerr    NZ     335\n",
       "5  Ashleigh Gardner   AUS     269\n",
       "6     Deepti Sharma   IND     249\n",
       "7     Jess Jonassen   AUS     245\n",
       "8         Sune Luus    SA     223\n",
       "9   Katherine Brunt   ENG     221"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=top_10_women_all_rounder('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba7dc9",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "982309b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnbc_news_headline(url):\n",
    "    Headline=[]\n",
    "    Time=[]\n",
    "    Link=[]\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    \n",
    "    headline=soup.find_all('a',class_ = \"LatestNews-headline\")\n",
    "    for i in headline:\n",
    "        Headline.append(i.text)\n",
    "        \n",
    "    time = soup.find_all('span',class_=\"LatestNews-wrapper\")\n",
    "    for i in time:\n",
    "        Time.append(i.text)\n",
    "        \n",
    "    link = soup.find_all('div',class_ = \"LatestNews-headlineWrapper\")\n",
    "    for i in link:\n",
    "        Link.append(i.a['href'])\n",
    "        \n",
    "        \n",
    "    import pandas as pd\n",
    "    cnbc_news=pd.DataFrame({})\n",
    "    cnbc_news['Headline']=Headline\n",
    "    cnbc_news['Time']=Time\n",
    "    cnbc_news['Link']=Link\n",
    "    \n",
    "    return(cnbc_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb3e27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla is cutting about 200 Autopilot jobs and ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/tesla-cutting-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Have stocks bottomed? Here’s one indicator pro...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goldman sees 'rapid' growth in semiconductors ...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hong Kong's Hang Seng leads losses as Asia Pac...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/29/asia-markets-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cramer's lightning round: I can't recommend Si...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AWS CEO says the move to cloud computing is on...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/aws-ceo-says-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U.S. to deploy nearly 300,000 monkeypox vaccin...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/us-to-deploy-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nike has a ‘much better risk-reward’ than the ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/cramer-nike-ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jim Cramer picks 4 'buyable' stocks to snap up...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/jim-cramer-pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stock futures are flat after failed attempt at...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/stock-market-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>China's economy didn't bounce back in the seco...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/chinas-economy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Best trades on CNBC Tuesday: Pros buy these st...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Key takeaways from explosive Jan. 6 hearing fe...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/jan-6-hearing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CVS removes purchase limit on Plan B pills, sa...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/cvs-to-remove-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FDA panel recommends changing Covid shots to f...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/fda-panel-reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Millions of US residents may get new stimulus ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/inflation-reli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>First phase of market downturn is done. Two mo...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/first-phase-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Disney extends CEO Bob Chapek's contract by th...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/disney-board-v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pinterest CEO Ben Silbermann is stepping down ...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/pinterest-shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Trump thought Pence 'deserves' chants of 'hang...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/jan-6-hearing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Apple using Qualcomm's 5G chip in next iPhone ...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>/investingclub/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Amazon limits how many Plan B pills you can bu...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/amazon-limits-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>California plans 'inflation relief' checks. Wi...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/california-pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Morgan Stanley's big buyback and dividend hike...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>/investingclub/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Intel CEO vents about Ohio chip plant delay: '...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/intel-ceo-on-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NATO strikes deal with Turkey to admit Sweden ...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/nato-reaches-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>UBS says this economy is going through 'slowfl...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Job loss is a 'reality' of the business cycle:...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/key-financial-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Consider hiding out in these tech stocks durin...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Trump lunged at Secret Service agent in rage w...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/06/28/trump-lunged-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline          Time  \\\n",
       "0   Tesla is cutting about 200 Autopilot jobs and ...   2 Hours Ago   \n",
       "1   Have stocks bottomed? Here’s one indicator pro...   3 Hours Ago   \n",
       "2   Goldman sees 'rapid' growth in semiconductors ...   3 Hours Ago   \n",
       "3   Hong Kong's Hang Seng leads losses as Asia Pac...   4 Hours Ago   \n",
       "4   Cramer's lightning round: I can't recommend Si...   4 Hours Ago   \n",
       "5   AWS CEO says the move to cloud computing is on...   4 Hours Ago   \n",
       "6   U.S. to deploy nearly 300,000 monkeypox vaccin...   5 Hours Ago   \n",
       "7   Nike has a ‘much better risk-reward’ than the ...   5 Hours Ago   \n",
       "8   Jim Cramer picks 4 'buyable' stocks to snap up...   5 Hours Ago   \n",
       "9   Stock futures are flat after failed attempt at...   6 Hours Ago   \n",
       "10  China's economy didn't bounce back in the seco...   6 Hours Ago   \n",
       "11  Best trades on CNBC Tuesday: Pros buy these st...   6 Hours Ago   \n",
       "12  Key takeaways from explosive Jan. 6 hearing fe...   6 Hours Ago   \n",
       "13  CVS removes purchase limit on Plan B pills, sa...   7 Hours Ago   \n",
       "14  FDA panel recommends changing Covid shots to f...   7 Hours Ago   \n",
       "15  Millions of US residents may get new stimulus ...   7 Hours Ago   \n",
       "16  First phase of market downturn is done. Two mo...   7 Hours Ago   \n",
       "17  Disney extends CEO Bob Chapek's contract by th...   7 Hours Ago   \n",
       "18  Pinterest CEO Ben Silbermann is stepping down ...   8 Hours Ago   \n",
       "19  Trump thought Pence 'deserves' chants of 'hang...   8 Hours Ago   \n",
       "20  Apple using Qualcomm's 5G chip in next iPhone ...   8 Hours Ago   \n",
       "21  Amazon limits how many Plan B pills you can bu...   8 Hours Ago   \n",
       "22  California plans 'inflation relief' checks. Wi...   8 Hours Ago   \n",
       "23  Morgan Stanley's big buyback and dividend hike...   8 Hours Ago   \n",
       "24  Intel CEO vents about Ohio chip plant delay: '...   9 Hours Ago   \n",
       "25  NATO strikes deal with Turkey to admit Sweden ...   9 Hours Ago   \n",
       "26  UBS says this economy is going through 'slowfl...   9 Hours Ago   \n",
       "27  Job loss is a 'reality' of the business cycle:...   9 Hours Ago   \n",
       "28  Consider hiding out in these tech stocks durin...  10 Hours Ago   \n",
       "29  Trump lunged at Secret Service agent in rage w...  10 Hours Ago   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://www.cnbc.com/2022/06/28/tesla-cutting-...  \n",
       "1                                               /pro/  \n",
       "2                                               /pro/  \n",
       "3   https://www.cnbc.com/2022/06/29/asia-markets-w...  \n",
       "4   https://www.cnbc.com/2022/06/28/cramers-lightn...  \n",
       "5   https://www.cnbc.com/2022/06/28/aws-ceo-says-t...  \n",
       "6   https://www.cnbc.com/2022/06/28/us-to-deploy-n...  \n",
       "7   https://www.cnbc.com/2022/06/28/cramer-nike-ha...  \n",
       "8   https://www.cnbc.com/2022/06/28/jim-cramer-pic...  \n",
       "9   https://www.cnbc.com/2022/06/28/stock-market-f...  \n",
       "10  https://www.cnbc.com/2022/06/28/chinas-economy...  \n",
       "11                                              /pro/  \n",
       "12  https://www.cnbc.com/2022/06/28/jan-6-hearing-...  \n",
       "13  https://www.cnbc.com/2022/06/28/cvs-to-remove-...  \n",
       "14  https://www.cnbc.com/2022/06/28/fda-panel-reco...  \n",
       "15  https://www.cnbc.com/2022/06/28/inflation-reli...  \n",
       "16  https://www.cnbc.com/2022/06/28/first-phase-of...  \n",
       "17  https://www.cnbc.com/2022/06/28/disney-board-v...  \n",
       "18  https://www.cnbc.com/2022/06/28/pinterest-shar...  \n",
       "19  https://www.cnbc.com/2022/06/28/jan-6-hearing-...  \n",
       "20                                    /investingclub/  \n",
       "21  https://www.cnbc.com/2022/06/28/amazon-limits-...  \n",
       "22  https://www.cnbc.com/2022/06/28/california-pla...  \n",
       "23                                    /investingclub/  \n",
       "24  https://www.cnbc.com/2022/06/28/intel-ceo-on-o...  \n",
       "25  https://www.cnbc.com/2022/06/28/nato-reaches-d...  \n",
       "26                                              /pro/  \n",
       "27  https://www.cnbc.com/2022/06/28/key-financial-...  \n",
       "28                                              /pro/  \n",
       "29  https://www.cnbc.com/2022/06/28/trump-lunged-a...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=cnbc_news_headline('https://www.cnbc.com/world/?region=world')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d85ba",
   "metadata": {},
   "source": [
    "# 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee3e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AI(url):\n",
    "    Title=[]\n",
    "    Authors=[]\n",
    "    Date=[]\n",
    "    Link=[]\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    \n",
    "    title=soup.find_all('h2',class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\")\n",
    "    for i in title:\n",
    "        Title.append(i.text)\n",
    "        \n",
    "    authors = soup.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\")\n",
    "    for i in authors:\n",
    "        Authors.append(i.text)\n",
    "        \n",
    "    date = soup.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\")\n",
    "    for i in date:\n",
    "        Date.append(i.text)\n",
    "        \n",
    "    link = soup.find_all('li',class_=\"sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp\")\n",
    "    for i in link:\n",
    "        Link.append(i.a['href'])\n",
    "        \n",
    "        \n",
    "    import pandas as pd\n",
    "    AI=pd.DataFrame({})\n",
    "    AI['Paper Title']=Title\n",
    "    AI['Authors']=Authors\n",
    "    AI['Published Date']=Date\n",
    "    AI['Paper URL']=Link\n",
    "    \n",
    "    return(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ea1c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=AI('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77312b3a",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9aaebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dineout(url):\n",
    "    Name=[]\n",
    "    Cuisine=[]\n",
    "    Location=[]\n",
    "    Ratings=[]\n",
    "    Image=[]\n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    \n",
    "    name=soup.find_all('div',class_=\"restnt-info cursor\")\n",
    "    for i in name:\n",
    "        Name.append(i.text)\n",
    "        \n",
    "    cus = soup.find_all('span',class_=\"double-line-ellipsis\")\n",
    "    for i in cus:\n",
    "        Cuisine.append(i.text.split('|')[1])\n",
    "        \n",
    "    loc = soup.find_all('div',class_=\"restnt-loc ellipsis\")\n",
    "    for i in loc:\n",
    "        Location.append(i.text)\n",
    "        \n",
    "    rating = soup.find_all('div',class_=\"restnt-rating rating-4\")\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "    image = soup.find_all('img',class_=\"no-img\")\n",
    "    for i in image:\n",
    "        Image.append(i['data-src'])\n",
    "        \n",
    "        \n",
    "    import pandas as pd\n",
    "    Dineout=pd.DataFrame({})\n",
    "    Dineout['Restaurant name']=Name\n",
    "    Dineout['Cuisine']=Cuisine\n",
    "    Dineout['Location']=Location\n",
    "    Dineout['Ratings']=Ratings\n",
    "    Dineout['Image URL']=Image\n",
    "    \n",
    "    return(Dineout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07a56440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle BarbequeConnaught Place, Central Delhi</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree3CS Mall,Lajpat Nagar - 3, Sout...</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle BarbequePacific Mall,Tagore Garden, Wes...</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe KnoshThe Leela Ambience Convention Hotel,...</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque CompanyGardens Galleria,Sector 38...</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India GrillHilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi BarbequeTaurus Sarovar Portico,Mahipalpu...</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que VillageIndirapuram Ha...</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>World CafeVibe by The Lalit Traveller,Sector 3...</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Vibe by The Lalit Traveller,Sector 35, Faridabad</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indian Grill RoomSuncity Business Tower,Golf C...</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mad 4 Bar B QueSector 29, Faridabad</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Sector 29, Faridabad</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbeque 29NIT, Faridabad</td>\n",
       "      <td>North Indian, Mughlai, Desserts, Beverages</td>\n",
       "      <td>NIT, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GlasshouseDoubleTree By Hilton Gurugram Baani ...</td>\n",
       "      <td>European, Italian, Asian, Continental</td>\n",
       "      <td>DoubleTree By Hilton Gurugram Baani Square,Sec...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Restaurant name  \\\n",
       "0       Castle BarbequeConnaught Place, Central Delhi   \n",
       "1   Jungle Jamboree3CS Mall,Lajpat Nagar - 3, Sout...   \n",
       "2   Castle BarbequePacific Mall,Tagore Garden, Wes...   \n",
       "3   Cafe KnoshThe Leela Ambience Convention Hotel,...   \n",
       "4   The Barbeque CompanyGardens Galleria,Sector 38...   \n",
       "5     India GrillHilton Garden Inn,Saket, South Delhi   \n",
       "6   Delhi BarbequeTaurus Sarovar Portico,Mahipalpu...   \n",
       "7   The Monarch - Bar Be Que VillageIndirapuram Ha...   \n",
       "8   World CafeVibe by The Lalit Traveller,Sector 3...   \n",
       "9   Indian Grill RoomSuncity Business Tower,Golf C...   \n",
       "10                Mad 4 Bar B QueSector 29, Faridabad   \n",
       "11                          Barbeque 29NIT, Faridabad   \n",
       "12  GlasshouseDoubleTree By Hilton Gurugram Baani ...   \n",
       "\n",
       "                                        Cuisine  \\\n",
       "0                         North Indian, Chinese   \n",
       "1                  North Indian, Asian, Italian   \n",
       "2                         Chinese, North Indian   \n",
       "3                          Italian, Continental   \n",
       "4                         North Indian, Chinese   \n",
       "5                         North Indian, Italian   \n",
       "6                                  North Indian   \n",
       "7                                  North Indian   \n",
       "8                         North Indian, Italian   \n",
       "9                         North Indian, Mughlai   \n",
       "10                                 North Indian   \n",
       "11   North Indian, Mughlai, Desserts, Beverages   \n",
       "12        European, Italian, Asian, Continental   \n",
       "\n",
       "                                             Location Ratings  \\\n",
       "0                      Connaught Place, Central Delhi     3.5   \n",
       "1              3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2              Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "3   The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "4                  Gardens Galleria,Sector 38A, Noida       4   \n",
       "5                Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6      Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "7   Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8    Vibe by The Lalit Traveller,Sector 35, Faridabad     4.3   \n",
       "9    Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "10                               Sector 29, Faridabad     3.6   \n",
       "11                                     NIT, Faridabad     4.2   \n",
       "12  DoubleTree By Hilton Gurugram Baani Square,Sec...       4   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=Dineout('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83596a5",
   "metadata": {},
   "source": [
    "# 10) Write a python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank\n",
    "ii) Publication\n",
    "iii) h5-index\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0589c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scholar(url):\n",
    "    Rank=[]\n",
    "    Publication=[]\n",
    "    Index=[]\n",
    "    Median=[]\n",
    "    \n",
    "    \n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.content,'html.parser')\n",
    "    \n",
    "    rank=soup.find_all('td',class_=\"gsc_mvt_p\")\n",
    "    for i in rank:\n",
    "        Rank.append(i.text)\n",
    "        \n",
    "    publ = soup.find_all('td',class_=\"gsc_mvt_t\")\n",
    "    for i in publ:\n",
    "        Publication.append(i.text)\n",
    "        \n",
    "    index = soup.find_all('a',class_=\"gs_ibl gsc_mp_anchor\")\n",
    "    for i in index:\n",
    "        Index.append(i.text)\n",
    "        \n",
    "    median = soup.find_all('span',class_=\"gs_ibl gsc_mp_anchor\")\n",
    "    for i in median:\n",
    "        Median.append(i.text)\n",
    "        \n",
    "           \n",
    "        \n",
    "    import pandas as pd\n",
    "    Scholar=pd.DataFrame({})\n",
    "    Scholar['Rank']=Rank\n",
    "    Scholar['Publication']=Publication\n",
    "    Scholar['h5-index']=Index\n",
    "    Scholar['h5-median']=Median\n",
    "   \n",
    "    \n",
    "    return(Scholar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a49506d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5-index h5-median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=Scholar('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
